2024-04-30 02:00:50,139 - INFO - Config:
2024-04-30 02:00:50,140 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/MIMIC/LoS/TransformerMSE",
    "batch_size": 32,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "d_model": 16,
    "dataset": "MIMIC",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "TransformerMSE",
    "feedforward_size": 256,
    "intermediate_reporting": false,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00017,
    "loss": "msle",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "n_epochs": 15,
    "n_heads": 2,
    "n_layers": 6,
    "name": "TransformerMSE",
    "no_diag": true,
    "no_exp": false,
    "no_labs": true,
    "no_mask": false,
    "percentage_data": 100.0,
    "positional_encoding": false,
    "save_results_csv": false,
    "seed": 358672992,
    "shuffle_train": false,
    "sum_losses": true,
    "task": "LoS",
    "trans_dropout_rate": 0
}
2024-04-30 02:00:51,563 - INFO - Experiment set up.
2024-04-30 02:00:51,803 - INFO - Transformer(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (trans_dropout): Dropout(p=0, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (transformer): TransformerEncoder(
    (input_embedding): Conv1d(204, 16, kernel_size=(1,), stride=(1,))
    (pos_encoder): PositionalEncoding()
    (trans_encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
      )
      (linear1): Linear(in_features=16, out_features=256, bias=True)
      (dropout): Dropout(p=0, inplace=False)
      (linear2): Linear(in_features=256, out_features=16, bias=True)
      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
  )
  (diagnosis_encoder): Linear(in_features=1, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=49, out_features=17, bias=True)
  (point_mort): Linear(in_features=49, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
2024-04-30 02:11:21,809 - INFO - Custom bins confusion matrix:
2024-04-30 02:11:21,811 - INFO - [[334527 415495 108402  45844  23563  13523   8345   5350  10857   2652]
 [149050 261802  95754  46784  26449  15779  10418   7112  14788   3647]
 [ 73270 161089  71105  38355  22933  14546   9680   6725  14652   4058]
 [ 41159 106448  54304  31429  19365  12713   8735   6019  13779   3832]
 [ 26504  75180  42466  25662  16760  11358   7441   5237  12165   3428]
 [ 18175  56161  34200  21277  14096   9504   6597   4599  10804   3262]
 [ 13236  43272  28372  18458  12312   8213   5977   4199   9548   2810]
 [  9988  34500  23894  15760  10484   7285   5146   3682   8440   2653]
 [ 29511 111926  84245  58605  40962  28542  20089  14368  34511  10896]
 [ 17221  83177  74745  57008  40750  29958  21599  15983  39491  13725]]
2024-04-30 02:11:25,802 - INFO - Epoch: 0 | Train Loss: 143.5463
2024-04-30 02:13:31,328 - INFO - Custom bins confusion matrix:
2024-04-30 02:13:31,328 - INFO - [[75810 80634 29717 13610  6180  2248   852   226    62     1]
 [29398 51881 27007 15219  8623  3747  1309   351   105     0]
 [12508 29993 20241 13661  8656  4065  1605   443   180     0]
 [ 6332 18532 14641 11107  7953  4194  1644   417   167     0]
 [ 3458 12126 11123  8848  6685  3928  1641   404   139     0]
 [ 2031  7991  8408  7021  5883  3443  1553   327   132     0]
 [ 1311  5935  6367  5394  5188  2975  1534   310   118     0]
 [  964  4489  4990  4556  4412  2927  1390   321   109     0]
 [ 2092 12360 17144 17023 17414 12879  5493  1674   551     0]
 [ 1139  6087  9974 13679 15498 13818  7763  2816  1267     0]]
2024-04-30 02:13:32,772 - INFO - Epoch: 0 | Validation Loss: 113.1103
2024-04-30 02:23:23,513 - INFO - Custom bins confusion matrix:
2024-04-30 02:23:23,514 - INFO - [[410748 347893 104130  45977  23105  12818   7676   4895   9277   2039]
 [159147 236657 101859  51937  28859  17334  10813   7038  14502   3437]
 [ 69090 145445  78088  43413  26079  16257  10652   7318  15847   4224]
 [ 34311  94904  58969  36015  22382  14704   9966   6760  15400   4372]
 [ 20011  65493  45777  29229  18923  12893   8917   6090  14603   4265]
 [ 12253  47857  36597  24358  16415  10957   7704   5567  12953   4014]
 [  8427  35992  29916  20770  13990   9998   6859   4956  11817   3672]
 [  5870  28141  24827  17609  12096   8755   5979   4399  10633   3523]
 [ 16400  86032  84189  64186  46658  33872  24334  17927  44607  15450]
 [  8235  58908  70380  59148  45383  34074  25396  19228  52115  20790]]
2024-04-30 02:23:27,363 - INFO - Epoch: 1 | Train Loss: 127.1884
2024-04-30 02:25:30,490 - INFO - Custom bins confusion matrix:
2024-04-30 02:25:30,490 - INFO - [[87090 70917 27842 13423  5763  2477  1151   473   204     0]
 [32786 48189 25649 15500  7958  4334  2005   881   338     0]
 [13620 27595 19943 12986  8388  4759  2456  1018   587     0]
 [ 6631 17055 14393 10482  7328  4655  2634  1226   583     0]
 [ 3631 11068 10571  8157  6187  4423  2524  1255   536     0]
 [ 2105  7229  7647  6496  5332  3885  2396  1164   535     0]
 [ 1385  5256  5869  4968  4264  3484  2238  1071   597     0]
 [ 1042  3811  4701  4112  3583  2997  2307  1065   540     0]
 [ 2163 10238 15691 14081 14708 12695  9905  4402  2747     0]
 [ 1261  4866  8767 10947 11133 12022 11471  6384  5190     0]]
2024-04-30 02:25:32,249 - INFO - Epoch: 1 | Validation Loss: 107.8043
2024-04-30 02:35:20,034 - INFO - Custom bins confusion matrix:
2024-04-30 02:35:20,035 - INFO - [[415358 349147 103077  44990  22198  12194   7151   4344   8307   1792]
 [153956 238854 104212  53586  29355  17270  10567   6857  13682   3244]
 [ 64468 144325  80550  45058  27116  16992  10843   7445  15594   4022]
 [ 31021  92093  60792  37617  23699  15114   9987   7139  15816   4505]
 [ 17637  62624  46818  30342  20184  13255   9208   6460  15083   4590]
 [ 10632  44922  37003  25299  17028  11692   8169   5741  13919   4270]
 [  7042  33511  29989  21429  14817  10476   7235   5172  12633   4093]
 [  4918  25727  24895  18245  12776   8788   6416   4628  11598   3841]
 [ 13548  76997  82089  65560  48383  35619  26183  19176  48674  17426]
 [  7031  50720  66727  57925  45853  35426  27017  20838  57464  24656]]
2024-04-30 02:35:23,840 - INFO - Epoch: 2 | Train Loss: 121.8091
2024-04-30 02:37:26,421 - INFO - Custom bins confusion matrix:
2024-04-30 02:37:26,422 - INFO - [[90393 68994 25664 13199  6301  2625  1322   517   325     0]
 [33379 46976 24657 15613  8442  4451  2408  1072   642     0]
 [13589 26952 18751 13187  8275  5357  2746  1498   997     0]
 [ 6600 16313 13588 10489  7252  4887  2939  1811  1107     1]
 [ 3502 10545  9771  7976  6387  4382  2844  1847  1098     0]
 [ 2061  6840  6925  6242  5243  4047  2636  1634  1161     0]
 [ 1276  5036  5300  4812  4086  3305  2557  1553  1207     0]
 [  995  3648  4120  4087  3282  2946  2417  1578  1085     0]
 [ 2110  9453 13378 13857 13233 12006 10633  6860  5100     0]
 [ 1259  4427  7611  9727  9643 10720 10271  9364  9019     0]]
2024-04-30 02:37:28,227 - INFO - Epoch: 2 | Validation Loss: 104.7169
2024-04-30 02:47:16,843 - INFO - Custom bins confusion matrix:
2024-04-30 02:47:16,844 - INFO - [[401286 370896 100042  43283  21255  11537   6871   4119   7713   1556]
 [142320 249964 105956  53814  29108  17186  10463   6705  13112   2955]
 [ 58005 147690  81350  46507  27605  17295  11074   7329  15636   3922]
 [ 27319  92405  61533  38379  24032  15442  10630   7251  16194   4598]
 [ 15359  61826  46626  31565  20661  13677   9634   6718  15462   4673]
 [  8983  43792  36816  25697  17667  12174   8596   6086  14300   4564]
 [  5919  32445  29462  22047  15258  10607   7579   5440  13234   4406]
 [  4010  24612  24719  18484  12934   9257   6609   4783  12248   4176]
 [ 11436  72221  80133  65620  48798  36922  27126  20278  52203  18918]
 [  5955  45875  63509  57502  46095  36555  28201  21792  61057  27116]]
2024-04-30 02:47:21,774 - INFO - Epoch: 3 | Train Loss: 118.1341
2024-04-30 02:49:24,326 - INFO - Custom bins confusion matrix:
2024-04-30 02:49:24,327 - INFO - [[92231 69754 24182 12317  6093  2558  1214   629   362     0]
 [33312 47945 24027 15162  8401  4274  2426  1213   880     0]
 [13334 27681 17990 12736  8413  5183  3013  1655  1347     0]
 [ 6380 16678 13130 10027  7288  4607  3179  2063  1633     2]
 [ 3375 10668  9401  7784  6133  4303  3014  1995  1679     0]
 [ 1996  6837  6672  5858  5168  3866  2788  1839  1758     7]
 [ 1214  5178  4877  4597  4014  3070  2549  1892  1741     0]
 [  930  3807  3785  3949  3066  2803  2318  1845  1655     0]
 [ 2122  9563 12468 13236 12468 11332 10308  7404  7715    14]
 [ 1277  4549  7000  8945  9075  9691  9388  9135 12981     0]]
2024-04-30 02:49:26,155 - INFO - Epoch: 3 | Validation Loss: 102.5021
2024-04-30 02:59:16,719 - INFO - Custom bins confusion matrix:
2024-04-30 02:59:16,720 - INFO - [[395587 381073  98770  42278  20663  11205   6506   3926   7124   1426]
 [135888 255832 107213  53825  29381  16885  10333   6572  12827   2827]
 [ 54283 149012  82754  47457  27722  17223  11294   7382  15471   3815]
 [ 25051  92383  61737  39382  24380  16039  10697   7262  16440   4412]
 [ 14089  60845  46997  32026  21011  14084   9739   6930  15889   4591]
 [  8273  42670  36604  26327  17958  12542   8687   6141  14790   4683]
 [  5448  31281  29560  22001  15603  10867   7772   5602  13686   4577]
 [  3696  23730  24089  18624  13122   9533   6989   5072  12647   4330]
 [ 10667  68124  78548  65656  50343  37471  27687  20911  54067  20181]
 [  5650  41855  60924  56995  46593  37257  28676  22607  64134  28966]]
2024-04-30 02:59:20,521 - INFO - Epoch: 4 | Train Loss: 115.4793
2024-04-30 03:01:23,302 - INFO - Custom bins confusion matrix:
2024-04-30 03:01:23,303 - INFO - [[90505 72800 23260 12061  5871  2638  1188   569   448     0]
 [31663 49708 23781 14849  8472  4430  2381  1284  1072     0]
 [12515 28302 17596 12481  8648  5155  3087  1910  1658     0]
 [ 5861 17212 12612  9635  7339  4662  3273  2231  2160     2]
 [ 3098 10883  9028  7485  6108  4301  3063  2173  2211     2]
 [ 1837  6887  6410  5610  5151  3778  2870  1933  2300    13]
 [ 1121  5230  4594  4415  3992  3009  2502  1894  2369     6]
 [  874  3903  3466  3762  3095  2673  2365  1841  2176     3]
 [ 1996  9639 11872 12633 12020 10964  9701  7819  9942    44]
 [ 1260  4599  6437  8499  8726  8867  9074  8098 16290   191]]
2024-04-30 03:01:25,083 - INFO - Epoch: 4 | Validation Loss: 100.9469
2024-04-30 03:11:11,538 - INFO - Custom bins confusion matrix:
2024-04-30 03:11:11,539 - INFO - [[391456 388737  98225  41295  19978  10697   6203   3806   6814   1347]
 [132218 259525 107952  54214  29255  16686  10259   6483  12298   2693]
 [ 51915 149977  83115  47907  28414  17460  11044   7452  15512   3617]
 [ 23714  92156  62381  39379  24990  16140  10800   7384  16432   4407]
 [ 13446  59768  47101  32558  21460  14406   9830   7032  15973   4627]
 [  7789  41822  36802  26450  18137  12552   8944   6283  15206   4690]
 [  5150  30318  29257  22153  15877  11032   7935   5897  14183   4595]
 [  3457  22849  24006  18728  13469   9570   7036   5146  13110   4461]
 [ 10240  64728  76949  65472  50707  38370  28603  21591  56162  20833]
 [  5293  38663  58879  56442  46713  37656  29887  23168  66462  30494]]
2024-04-30 03:11:15,702 - INFO - Epoch: 5 | Train Loss: 113.3089
2024-04-30 03:13:17,946 - INFO - Custom bins confusion matrix:
2024-04-30 03:13:17,948 - INFO - [[89013 74986 23464 11583  5702  2509  1124   503   456     0]
 [30418 50982 24345 14536  8412  4277  2341  1244  1085     0]
 [11919 28714 17774 12473  8768  4983  3149  1852  1717     3]
 [ 5458 17555 12649  9467  7474  4589  3284  2211  2298     2]
 [ 2917 11035  8969  7380  6123  4309  3081  2175  2362     1]
 [ 1712  6956  6427  5498  5092  3826  2874  1902  2488    14]
 [ 1051  5233  4587  4399  3905  3057  2501  1796  2587    16]
 [  834  3884  3497  3697  3070  2690  2307  1777  2396     6]
 [ 1855  9723 11750 12431 11838 10922  9541  7868 10638    64]
 [ 1220  4584  6298  8415  8488  8796  8752  7952 17076   460]]
2024-04-30 03:13:19,745 - INFO - Epoch: 5 | Validation Loss: 99.8978
2024-04-30 03:23:09,383 - INFO - Custom bins confusion matrix:
2024-04-30 03:23:09,384 - INFO - [[387509 395111  97875  40589  19749  10464   5887   3657   6435   1282]
 [129338 262126 108751  54432  28940  16873  10007   6501  12057   2558]
 [ 50242 150170  84271  48489  28538  17349  11121   7430  15244   3559]
 [ 22821  91370  62743  39821  25285  16467  10978   7602  16499   4197]
 [ 12637  59078  47361  32704  21738  14897  10080   7171  15988   4547]
 [  7431  40915  36589  26722  18613  12910   9104   6484  15213   4694]
 [  4961  29320  29510  22388  15927  11406   8019   5880  14384   4602]
 [  3372  22057  23838  18820  13604   9924   7413   5248  13053   4503]
 [  9978  61823  75581  65866  51189  39000  29329  22323  57424  21142]
 [  5042  36169  56655  56280  47117  38148  30314  23588  68712  31632]]
2024-04-30 03:23:13,453 - INFO - Epoch: 6 | Train Loss: 111.6195
2024-04-30 03:25:15,754 - INFO - Custom bins confusion matrix:
2024-04-30 03:25:15,755 - INFO - [[90436 75335 23196 10799  5163  2380  1113   467   451     0]
 [30782 51697 24396 13985  7935  4214  2361  1166  1103     1]
 [11979 29199 17829 12242  8459  4927  3131  1798  1784     4]
 [ 5484 17948 12569  9207  7315  4641  3247  2132  2442     2]
 [ 2924 11357  8857  7247  5932  4312  3045  2161  2510     7]
 [ 1736  7115  6411  5376  4949  3756  2872  1875  2683    16]
 [ 1072  5347  4484  4362  3824  3059  2466  1740  2749    29]
 [  868  3907  3512  3677  2889  2666  2348  1668  2614     9]
 [ 1888 10096 11649 12032 11640 10661  9423  7774 11401    66]
 [ 1261  4714  6289  8265  7994  8691  8577  7818 17688   744]]
2024-04-30 03:25:17,550 - INFO - Epoch: 6 | Validation Loss: 98.9591
2024-04-30 03:35:02,591 - INFO - Custom bins confusion matrix:
2024-04-30 03:35:02,592 - INFO - [[384563 399730  98179  40225  18924  10070   5763   3571   6350   1183]
 [127272 263687 109702  54301  29343  16794  10133   6297  11646   2408]
 [ 49095 150327  84614  49080  28789  17493  11188   7514  14896   3417]
 [ 22023  90743  63119  40132  25431  16673  11208   7795  16505   4154]
 [ 12273  58295  47295  33250  22157  14886  10255   7154  16147   4489]
 [  7205  40066  36491  27195  18824  13115   9269   6579  15408   4523]
 [  4690  28725  29265  22443  16146  11534   8295   6003  14754   4542]
 [  3231  21492  23522  18805  14013  10138   7311   5314  13455   4551]
 [  9760  59102  74246  65647  52247  39715  29884  22700  58996  21358]
 [  4961  33583  54859  54970  47528  38793  31137  24285  71223  32318]]
2024-04-30 03:35:06,812 - INFO - Epoch: 7 | Train Loss: 110.1543
2024-04-30 03:37:09,570 - INFO - Custom bins confusion matrix:
2024-04-30 03:37:09,571 - INFO - [[88169 78354 23262 10343  4860  2337  1123   453   439     0]
 [29386 53362 24609 13726  7711  4238  2378  1120  1104     6]
 [11265 30112 17845 12177  8257  4976  3159  1804  1753     4]
 [ 5116 18419 12495  9148  7196  4703  3381  2075  2452     2]
 [ 2697 11642  8803  7232  5789  4328  3138  2230  2484     9]
 [ 1603  7332  6335  5303  4892  3765  2923  1898  2722    16]
 [  981  5410  4513  4334  3792  3033  2538  1699  2801    31]
 [  786  3955  3575  3598  2863  2700  2334  1684  2636    27]
 [ 1803 10310 11532 11616 11466 10604  9679  7860 11694    66]
 [ 1172  4905  6273  7844  7695  8760  8688  7854 18037   813]]
2024-04-30 03:37:11,386 - INFO - Epoch: 7 | Validation Loss: 98.2751
2024-04-30 03:46:56,297 - INFO - Custom bins confusion matrix:
2024-04-30 03:46:56,298 - INFO - [[382015 403788  98469  39333  18750   9778   5737   3433   6146   1109]
 [126106 264060 110439  54893  29153  16753  10013   6224  11657   2285]
 [ 48290 149732  85474  49209  28974  17640  11371   7466  14950   3307]
 [ 21646  89448  64037  40486  26153  16961  11222   7628  16264   3938]
 [ 11887  57487  47789  33453  22137  14924  10359   7376  16247   4542]
 [  6994  39281  36407  27385  19306  13373   9384   6614  15376   4555]
 [  4535  28049  29153  22757  16325  11757   8303   6137  14856   4525]
 [  3182  20750  23544  19077  14027  10245   7388   5611  13575   4433]
 [  9425  57138  73151  65710  52869  40554  30616  23017  59542  21633]
 [  4901  31511  52931  54652  47934  39044  31869  24643  73019  33153]]
2024-04-30 03:47:03,504 - INFO - Epoch: 8 | Train Loss: 108.9589
2024-04-30 03:49:06,426 - INFO - Custom bins confusion matrix:
2024-04-30 03:49:06,428 - INFO - [[87417 80113 22974  9915  4558  2346  1105   474   436     2]
 [28841 54242 24549 13498  7589  4264  2397  1139  1111    10]
 [10959 30556 17888 11998  8108  5010  3212  1822  1789    10]
 [ 4915 18769 12363  9062  7039  4778  3440  2060  2559     2]
 [ 2573 11896  8695  7077  5741  4288  3235  2220  2610    17]
 [ 1528  7532  6209  5254  4736  3763  2982  1974  2789    22]
 [  970  5466  4573  4126  3799  3008  2538  1726  2885    41]
 [  753  4005  3635  3436  2834  2643  2349  1742  2719    42]
 [ 1755 10524 11498 11156 11176 10449  9714  7931 12337    90]
 [ 1161  5014  6333  7488  7482  8568  8510  7846 18725   914]]
2024-04-30 03:49:08,237 - INFO - Epoch: 8 | Validation Loss: 97.6989
2024-04-30 03:58:57,725 - INFO - Custom bins confusion matrix:
2024-04-30 03:58:57,726 - INFO - [[383452 403310  99047  38676  18486   9763   5528   3369   5822   1105]
 [125990 263385 111773  54631  29467  16582   9989   6197  11347   2222]
 [ 48224 148229  86680  49433  29204  17866  11334   7491  14764   3188]
 [ 21586  88009  64553  40884  26195  17097  11365   7888  16298   3908]
 [ 11853  56291  47718  33716  22748  15286  10592   7345  16400   4252]
 [  6841  38452  36699  27377  19359  13626   9420   6820  15750   4331]
 [  4540  27215  29117  22767  16606  11986   8550   6247  14773   4596]
 [  3121  20124  23321  19050  14132  10443   7556   5660  13994   4431]
 [  9306  54915  72097  65828  53684  40857  31172  23330  60576  21890]
 [  4756  29897  51498  54202  47904  39568  31798  25157  74636  34241]]
2024-04-30 03:59:05,031 - INFO - Epoch: 9 | Train Loss: 107.8359
2024-04-30 04:01:07,763 - INFO - Custom bins confusion matrix:
2024-04-30 04:01:07,764 - INFO - [[85914 82088 22943  9605  4467  2337  1061   492   430     3]
 [27979 55117 24811 13300  7520  4257  2400  1160  1080    16]
 [10537 30797 18241 11758  8105  5056  3194  1845  1807    12]
 [ 4661 18949 12545  8850  7072  4795  3434  2141  2538     2]
 [ 2409 12043  8727  7009  5704  4303  3240  2270  2630    17]
 [ 1461  7554  6291  5165  4661  3788  3003  2052  2795    19]
 [  922  5462  4684  4045  3722  3033  2566  1746  2910    42]
 [  711  3990  3775  3279  2802  2723  2308  1758  2770    42]
 [ 1668 10528 11584 10904 10903 10554  9669  7978 12692   150]
 [ 1140  5137  6214  7238  7415  8499  8477  7906 19052   963]]
2024-04-30 04:01:09,556 - INFO - Epoch: 9 | Validation Loss: 97.2155
2024-04-30 04:10:55,422 - INFO - Custom bins confusion matrix:
2024-04-30 04:10:55,423 - INFO - [[383620 403303  99408  38650  18152   9809   5439   3318   5782   1077]
 [126333 260795 114123  54843  29747  16495   9843   6096  11173   2135]
 [ 48070 146739  87579  49860  29611  17867  11495   7497  14685   3010]
 [ 21370  86694  65044  41543  26719  17288  11448   7764  16209   3704]
 [ 11642  55002  48346  34076  23141  15640  10709   7337  16060   4248]
 [  6846  37426  36681  27609  19845  13875   9638   6805  15595   4355]
 [  4447  26422  29241  22839  16826  12219   8750   6284  15028   4341]
 [  3177  19386  23262  19385  14401  10572   7723   5669  13859   4398]
 [  9190  53032  71266  66043  54132  41540  31369  23762  61350  21971]
 [  4884  27811  49661  53458  48421  40020  32664  25857  76073  34808]]
2024-04-30 04:11:03,857 - INFO - Epoch: 10 | Train Loss: 106.8510
2024-04-30 04:13:05,594 - INFO - Custom bins confusion matrix:
2024-04-30 04:13:05,595 - INFO - [[85250 82430 23466  9461  4421  2331  1053   493   431     4]
 [27437 55141 25464 13210  7450  4306  2402  1173  1040    17]
 [10294 30574 18797 11671  8106  5105  3166  1832  1795    12]
 [ 4535 18788 12800  8880  7008  4830  3461  2186  2497     2]
 [ 2335 11909  8900  7018  5655  4358  3243  2276  2643    15]
 [ 1428  7451  6355  5201  4678  3759  3009  2082  2805    21]
 [  885  5372  4806  4024  3729  3054  2539  1795  2883    45]
 [  684  3916  3878  3257  2770  2794  2270  1761  2785    43]
 [ 1633 10410 11698 10807 10700 10691  9708  7966 12849   168]
 [ 1105  5134  6168  7115  7368  8468  8375  7963 19361   984]]
2024-04-30 04:13:07,393 - INFO - Epoch: 10 | Validation Loss: 96.7432
2024-04-30 04:22:54,384 - INFO - Custom bins confusion matrix:
2024-04-30 04:22:54,385 - INFO - [[383422 402990 100973  38422  18077   9377   5441   3232   5614   1010]
 [125628 259661 115880  55177  29620  16648   9834   6080  10960   2095]
 [ 47802 144891  89287  50378  29935  18086  11454   7389  14317   2874]
 [ 21506  84690  66295  41910  26643  17403  11593   7849  16244   3650]
 [ 11455  53752  48736  34498  23391  15764  10745   7484  16223   4153]
 [  6771  36250  37120  27681  19918  14026   9933   6928  15858   4190]
 [  4366  25663  29408  23231  17232  12169   8866   6144  15082   4236]
 [  3124  18734  23499  19093  14659  10730   7821   5747  14058   4367]
 [  9143  50585  70602  66182  54612  41864  31951  24207  62620  21889]
 [  4927  26093  48526  52976  48674  40264  32819  26295  77782  35301]]
2024-04-30 04:23:01,674 - INFO - Epoch: 11 | Train Loss: 105.9359
2024-04-30 04:25:03,845 - INFO - Custom bins confusion matrix:
2024-04-30 04:25:03,846 - INFO - [[85535 82184 23478  9311  4392  2250  1179   520   479    12]
 [27411 54894 25536 13117  7397  4252  2536  1316  1152    29]
 [10317 30283 18865 11548  7994  5013  3326  1939  2044    23]
 [ 4568 18564 12840  8682  6873  4864  3477  2286  2829     4]
 [ 2365 11766  8873  6865  5570  4282  3272  2333  3002    24]
 [ 1442  7362  6282  5142  4557  3686  2975  2170  3141    32]
 [  925  5211  4858  3871  3669  2997  2529  1862  3143    67]
 [  716  3823  3875  3221  2621  2758  2228  1781  3070    65]
 [ 1666 10253 11670 10497 10099 10430  9408  7941 14367   299]
 [ 1142  5130  6081  6848  7011  8003  7980  7785 20749  1312]]
2024-04-30 04:25:05,704 - INFO - Epoch: 11 | Validation Loss: 96.3252
2024-04-30 04:34:50,933 - INFO - Custom bins confusion matrix:
2024-04-30 04:34:50,933 - INFO - [[383493 402036 102886  38265  17758   9254   5230   3181   5506    949]
 [125502 257340 118767  55674  29336  16200   9879   6022  10859   2004]
 [ 47377 143024  91075  51050  29837  18077  11123   7543  14469   2838]
 [ 21291  83396  67172  42256  27080  17588  11651   7788  16040   3521]
 [ 11326  52461  49770  34485  23486  16057  10752   7614  16266   3984]
 [  6710  35215  37675  28302  20012  13969  10113   7031  15515   4133]
 [  4348  24878  29762  23216  17210  12462   8946   6303  15152   4120]
 [  3058  18142  23567  19337  14845  10688   7822   5826  14188   4359]
 [  9133  48704  70443  66178  55008  41991  32209  24530  63152  22307]
 [  4842  24781  47253  52885  48073  40510  33397  26550  79051  36315]]
2024-04-30 04:34:54,757 - INFO - Epoch: 12 | Train Loss: 105.1396
2024-04-30 04:36:56,815 - INFO - Custom bins confusion matrix:
2024-04-30 04:36:56,816 - INFO - [[87325 80136 23641  9292  4377  2274  1251   533   495    16]
 [28150 53728 25700 13192  7389  4266  2652  1339  1194    30]
 [10663 29594 18939 11585  8021  5047  3396  1991  2089    27]
 [ 4778 18058 12967  8688  6800  4877  3536  2377  2899     7]
 [ 2478 11485  8899  6783  5596  4311  3286  2370  3118    26]
 [ 1507  7185  6266  5109  4568  3695  2968  2202  3246    43]
 [  970  5057  4897  3814  3634  3034  2539  1890  3221    76]
 [  759  3695  3878  3192  2590  2792  2208  1827  3137    80]
 [ 1736  9964 11683 10272  9957 10360  9379  7984 14961   334]
 [ 1188  4988  6073  6594  6976  7845  7806  7611 21497  1463]]
2024-04-30 04:36:58,568 - INFO - Epoch: 12 | Validation Loss: 95.8907
2024-04-30 04:46:45,248 - INFO - Custom bins confusion matrix:
2024-04-30 04:46:45,249 - INFO - [[383306 401132 104723  38095  17590   9208   5051   3100   5394    959]
 [125576 255176 121085  56318  28974  16290   9765   5889  10635   1875]
 [ 47345 141129  92850  51317  29931  18264  11435   7240  14139   2763]
 [ 21098  81748  68672  42626  27240  17508  11606   7810  16012   3463]
 [ 11260  51187  50594  35186  23606  15875  11051   7492  16035   3915]
 [  6562  34456  38161  28439  20072  14096   9991   6960  15815   4123]
 [  4302  23988  29833  23662  17395  12558   8868   6411  15056   4324]
 [  3059  17500  23666  19588  15002  10979   7829   5741  14106   4362]
 [  9042  46920  69975  66149  55061  43173  32891  24687  63449  22308]
 [  4870  23505  46537  52231  48199  40681  33682  26807  80167  36978]]
2024-04-30 04:46:52,605 - INFO - Epoch: 13 | Train Loss: 104.4700
2024-04-30 04:48:54,534 - INFO - Custom bins confusion matrix:
2024-04-30 04:48:54,534 - INFO - [[85536 81469 24173  9158  4380  2258  1295   509   542    20]
 [27221 53980 26089 13213  7400  4274  2726  1432  1272    33]
 [10217 29455 19234 11557  8041  4949  3488  2073  2298    40]
 [ 4585 17875 13046  8660  6784  4869  3532  2453  3169    14]
 [ 2395 11246  9048  6674  5576  4288  3305  2406  3366    48]
 [ 1428  7108  6289  5083  4508  3671  2930  2163  3522    87]
 [  931  4964  4919  3790  3616  2989  2467  1900  3442   114]
 [  708  3670  3895  3123  2587  2748  2192  1757  3363   115]
 [ 1665  9683 11722 10123  9792 10132  9269  7728 16016   500]
 [ 1161  4865  6136  6410  6830  7600  7486  7403 22264  1886]]
2024-04-30 04:48:56,341 - INFO - Epoch: 13 | Validation Loss: 95.5821
2024-04-30 04:58:43,825 - INFO - Custom bins confusion matrix:
2024-04-30 04:58:43,826 - INFO - [[382208 400575 107096  38051  17377   9050   5043   2990   5255    913]
 [125559 252340 124335  56562  29207  16278   9404   5704  10357   1837]
 [ 47241 138722  95177  51839  30020  18095  11228   7344  14009   2738]
 [ 20912  80140  69882  43372  27296  17583  11673   7844  15694   3387]
 [ 10989  50275  51088  35409  23948  15924  10845   7453  16378   3892]
 [  6521  33522  38246  28923  20552  14075  10008   7167  15712   3949]
 [  4273  23196  29979  23937  17605  12538   9079   6338  15221   4231]
 [  3026  16856  23962  19633  15040  10932   8037   5746  14192   4408]
 [  8783  45346  69684  66566  55565  43272  32640  24794  64375  22630]
 [  4772  22444  45344  51554  48119  40949  33653  26859  81401  38562]]
2024-04-30 04:58:51,120 - INFO - Epoch: 14 | Train Loss: 103.7751
2024-04-30 05:00:53,937 - INFO - Custom bins confusion matrix:
2024-04-30 05:00:53,938 - INFO - [[87232 79162 24596  9314  4341  2256  1349   519   550    21]
 [27933 52406 26623 13427  7431  4296  2786  1465  1240    33]
 [10560 28553 19442 11708  8105  5061  3531  2077  2276    39]
 [ 4786 17277 13155  8747  6818  5014  3530  2465  3178    17]
 [ 2475 10827  9183  6703  5623  4393  3303  2388  3407    50]
 [ 1482  6839  6302  5168  4505  3747  2924  2172  3549   101]
 [  967  4756  4946  3835  3669  2985  2484  1915  3457   118]
 [  734  3474  3933  3156  2672  2675  2292  1699  3395   128]
 [ 1726  9161 11830 10186  9784 10111  9436  7715 16168   513]
 [ 1194  4634  6231  6228  6735  7692  7523  7347 22461  1996]]
2024-04-30 05:00:55,749 - INFO - Epoch: 14 | Validation Loss: 95.2581
2024-04-30 05:00:55,786 - INFO - Experiment ended. Checkpoints stored =)
