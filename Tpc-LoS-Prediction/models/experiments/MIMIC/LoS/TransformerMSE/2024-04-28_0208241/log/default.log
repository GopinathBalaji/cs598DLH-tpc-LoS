2024-04-28 02:08:24,090 - INFO - Config:
2024-04-28 02:08:24,090 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/MIMIC/LoS/TransformerMSE",
    "batch_size": 32,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "d_model": 16,
    "dataset": "MIMIC",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "TransformerMSE",
    "feedforward_size": 256,
    "intermediate_reporting": false,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00017,
    "loss": "mse",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "n_epochs": 15,
    "n_heads": 2,
    "n_layers": 6,
    "name": "TransformerMSE",
    "no_diag": true,
    "no_exp": false,
    "no_labs": false,
    "no_mask": false,
    "percentage_data": 100.0,
    "positional_encoding": false,
    "save_results_csv": false,
    "seed": 3725270571,
    "shuffle_train": false,
    "sum_losses": true,
    "task": "LoS",
    "trans_dropout_rate": 0
}
2024-04-28 02:08:25,520 - INFO - Experiment set up.
2024-04-28 02:08:25,854 - INFO - Transformer(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (trans_dropout): Dropout(p=0, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (transformer): TransformerEncoder(
    (input_embedding): Conv1d(204, 16, kernel_size=(1,), stride=(1,))
    (pos_encoder): PositionalEncoding()
    (trans_encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
      )
      (linear1): Linear(in_features=16, out_features=256, bias=True)
      (dropout): Dropout(p=0, inplace=False)
      (linear2): Linear(in_features=256, out_features=16, bias=True)
      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
  )
  (diagnosis_encoder): Linear(in_features=1, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=49, out_features=17, bias=True)
  (point_mort): Linear(in_features=49, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
2024-04-28 02:20:10,811 - INFO - Custom bins confusion matrix:
2024-04-28 02:20:10,813 - INFO - [[ 20800 299673 316598 157253  75249  38877  21396  12956  21946   3810]
 [ 12705 173532 194181 106832  56327  31995  18828  11734  21576   3873]
 [  7432 101794 121314  72114  41527  24392  15281   9869  19139   3551]
 [  5005  66076  81564  52558  31893  19733  12623   8375  16747   3209]
 [  3661  46840  59225  40171  25162  16142  10579   7173  14313   2935]
 [  2897  34772  45104  31868  20531  13562   8944   6018  12462   2517]
 [  2239  26971  35835  25983  17396  11489   7759   5220  11142   2363]
 [  1735  21251  29375  21619  14767   9974   6719   4586   9718   2088]
 [  5539  69322  99328  76813  53541  36780  25792  17887  39602   9051]
 [  3657  50372  80352  69003  51591  36875  26971  18971  44558  11307]]
2024-04-28 02:20:15,724 - INFO - Epoch: 0 | Train Loss: 8449.0812
2024-04-28 02:22:36,059 - INFO - Custom bins confusion matrix:
2024-04-28 02:22:36,060 - INFO - [[    12  40316 110604  33558  12781   7015   3360   1359    335      0]
 [     5  20541  63952  26282  13285   7806   4030   1397    342      0]
 [     0  10977  36177  18713  11909   7925   3902   1468    281      0]
 [     0   6104  22465  13618   9974   7403   3749   1427    247      0]
 [     0   3723  15009  10031   8032   6579   3537   1236    205      0]
 [     0   2349  10293   7257   6470   5789   3242   1181    208      0]
 [     0   1547   7540   5762   5047   5014   2968    998    256      0]
 [     0   1214   5613   4535   4517   4424   2754    898    203      0]
 [     0   2920  16265  16261  17368  17532  11890   3380   1014      0]
 [     0   1112   8607  10517  14445  17389  13730   4853   1388      0]]
2024-04-28 02:22:37,746 - INFO - Epoch: 0 | Validation Loss: 3552.1071
2024-04-28 02:34:07,483 - INFO - Custom bins confusion matrix:
2024-04-28 02:34:07,484 - INFO - [[   584 269889 366011 165902  73235  36293  19754  11935  21252   3703]
 [   260 146459 218938 115385  58366  32675  19508  12412  23159   4421]
 [   182  80852 132080  78369  44621  26521  16627  10895  21882   4384]
 [    88  49070  87243  57100  34610  21801  14214   9653  19829   4175]
 [    82  33251  61609  43578  27592  18075  12132   8269  17721   3892]
 [    72  23518  46347  33963  22814  15000  10473   7323  15548   3617]
 [    36  17571  35682  28107  19126  13119   9064   6392  14161   3139]
 [    39  13623  28564  23089  16142  11196   7874   5641  12680   2984]
 [    85  41328  92140  80182  59040  43038  30500  21900  51983  13459]
 [    54  25489  67316  68100  55093  42314  32245  24041  60528  18477]]
2024-04-28 02:34:12,516 - INFO - Epoch: 1 | Train Loss: 8137.9228
2024-04-28 02:36:32,555 - INFO - Custom bins confusion matrix:
2024-04-28 02:36:32,556 - INFO - [[    9 60486 96567 28532 11946  6876  3200  1395   328     1]
 [    8 30268 57272 23688 12870  7800  3854  1552   328     0]
 [    0 15671 33164 17059 11837  7688  3948  1633   352     0]
 [    0  8577 20870 12447  9990  7105  4003  1664   331     0]
 [    0  4925 14408  9026  8131  6274  3726  1504   358     0]
 [    0  3040  9955  6474  6583  5589  3417  1348   383     0]
 [    0  2078  7233  5068  5122  5052  3009  1224   346     0]
 [    0  1523  5398  4123  4432  4477  2797  1092   316     0]
 [    0  3649 15843 15302 16015 17536 12082  4798  1405     0]
 [    0  1695  7824  9516 12923 15958 14336  7674  2115     0]]
2024-04-28 02:36:34,333 - INFO - Epoch: 1 | Validation Loss: 3493.5717
2024-04-28 02:48:03,642 - INFO - Custom bins confusion matrix:
2024-04-28 02:48:03,643 - INFO - [[  2143 286545 352328 159719  72216  36935  20453  12481  21986   3752]
 [   889 149559 211721 113745  60445  33851  20170  12984  23880   4339]
 [   466  80271 128758  78099  45291  27742  17439  11433  22582   4332]
 [   249  47649  84280  57210  35568  22594  15069  10063  20762   4339]
 [   172  31766  59618  43283  28753  18658  12671   8665  18589   4026]
 [   107  21902  44556  34495  23347  15555  10865   7701  16420   3727]
 [    92  16355  34254  28066  19495  13790   9345   6698  14788   3514]
 [    79  12157  27433  23483  16702  11462   8213   5837  13272   3194]
 [   146  36754  87943  79313  61156  44256  32052  23124  54565  14346]
 [    39  20921  61726  66782  56023  43876  33295  24992  65340  20663]]
2024-04-28 02:48:08,739 - INFO - Epoch: 2 | Train Loss: 8033.9993
2024-04-28 02:50:29,128 - INFO - Custom bins confusion matrix:
2024-04-28 02:50:29,129 - INFO - [[   89 84719 62173 30434 17473  9303  3633  1227   287     2]
 [   67 39615 38795 25889 17115 10162  4366  1318   313     0]
 [   51 19809 22799 18457 14908  9480  4239  1305   304     0]
 [   23 10651 14535 13431 11908  9015  3941  1130   353     0]
 [   18  6457  9572  9972  9626  7744  3537  1096   330     0]
 [    1  3939  6560  7454  7509  6625  3282  1141   278     0]
 [    0  2788  4641  5768  5964  5565  3117  1016   273     0]
 [    0  1939  3673  4375  5091  5093  2779   962   246     0]
 [    8  4615 10475 15100 18342 20379 11871  4338  1502     0]
 [    0  2327  5289  7412 14569 19948 13981  6159  2356     0]]
2024-04-28 02:50:31,127 - INFO - Epoch: 2 | Validation Loss: 3475.9817
2024-04-28 03:02:03,799 - INFO - Custom bins confusion matrix:
2024-04-28 03:02:03,800 - INFO - [[  1883 294351 352109 155432  70074  36070  20471  12277  21866   4025]
 [   734 152409 213516 112164  58779  33153  19910  12697  23865   4356]
 [   371  81756 129619  77425  44943  26939  17354  11247  22360   4399]
 [   263  48266  84975  56945  35278  22317  14771   9998  20578   4392]
 [   180  31615  59604  43338  28546  18758  12728   8772  18543   4117]
 [   143  21740  44246  34228  23214  15942  10818   7492  16930   3922]
 [    85  15982  34299  27795  19819  13543   9446   6728  15075   3625]
 [    74  12131  27137  22858  16500  11896   8270   5991  13635   3340]
 [   175  35940  85655  79015  60153  44437  32635  23560  56305  15780]
 [    33  19596  59753  64839  55492  43386  33660  25441  67974  23483]]
2024-04-28 03:02:08,609 - INFO - Epoch: 3 | Train Loss: 7955.8974
2024-04-28 03:04:32,545 - INFO - Custom bins confusion matrix:
2024-04-28 03:04:32,546 - INFO - [[  725 74503 73103 33421 15695  7034  3179  1167   510     3]
 [  361 35962 44441 27038 15739  8152  4011  1352   584     0]
 [  206 17871 26310 19012 13582  8242  4077  1483   569     0]
 [  162  9726 16499 13543 11327  7784  3969  1408   569     0]
 [   69  5606 11352  9849  9058  6838  3765  1350   465     0]
 [   34  3453  7882  7143  7193  5811  3564  1290   419     0]
 [   13  2352  5606  5812  5643  4800  3238  1214   454     0]
 [   19  1704  4264  4571  4631  4493  2878  1112   486     0]
 [   35  4000 12226 15454 17393 17378 12907  5215  2022     0]
 [    0  1567  6815  7142 12190 17785 15115  8237  3190     0]]
2024-04-28 03:04:34,342 - INFO - Epoch: 3 | Validation Loss: 3442.2977
2024-04-28 03:16:06,004 - INFO - Custom bins confusion matrix:
2024-04-28 03:16:06,004 - INFO - [[  7681 319263 331524 148505  68546  35513  19941  12094  21462   4029]
 [  3231 163589 204239 109077  57545  32881  20095  12653  23794   4479]
 [  1515  86301 124974  75926  44361  27225  17406  11554  22619   4532]
 [   877  49954  82680  55671  35437  22703  14914  10076  20881   4590]
 [   573  32719  57931  42369  28373  18956  12870   8937  19144   4329]
 [   359  22654  43042  33453  22963  15959  11164   7701  17217   4163]
 [   242  16365  33209  27132  19417  13897   9733   6907  15542   3953]
 [   200  12154  26582  22576  16687  11772   8481   5999  13844   3537]
 [   431  35437  83289  76906  59827  44753  32829  23929  58970  17284]
 [   157  18981  56943  62983  54210  43441  33969  26061  70251  26661]]
2024-04-28 03:16:10,806 - INFO - Epoch: 4 | Train Loss: 7895.6893
2024-04-28 03:18:33,933 - INFO - Custom bins confusion matrix:
2024-04-28 03:18:33,934 - INFO - [[     0  49249 102321  32517  12175   6236   3494   1914   1434      0]
 [     1  24577  60142  25596  11999   7143   4214   2347   1621      0]
 [     0  12414  34710  17810  10732   6881   4367   2603   1835      0]
 [     0   6562  21760  12496   8776   6673   4133   2772   1815      0]
 [     0   3483  14860   9168   7120   5474   4000   2379   1868      0]
 [     0   1927  10196   6917   5614   4433   3767   2296   1639      0]
 [     0   1354   7385   5311   4413   3711   3267   2144   1547      0]
 [     0   1061   5347   4468   3499   3583   2860   1892   1448      0]
 [     1   2245  15848  15098  13106  13409  12207   8185   6531      0]
 [     0   1139   7859   8194  10441  11297  10929  11020  11162      0]]
2024-04-28 03:18:35,775 - INFO - Epoch: 4 | Validation Loss: 3358.0089
2024-04-28 03:30:08,994 - INFO - Custom bins confusion matrix:
2024-04-28 03:30:08,995 - INFO - [[  7630 289617 341556 159102  73075  37681  21191  12723  22064   3919]
 [  2921 148386 208197 115089  60749  33977  20578  12932  24275   4479]
 [  1534  77603 125988  79472  46501  28155  17934  11528  23078   4620]
 [   803  45084  82851  57945  36611  23236  15276  10393  21090   4494]
 [   486  29287  58094  43971  29124  19283  13166   9196  19175   4419]
 [   275  20169  42664  34542  23967  16297  11218   8099  17387   4057]
 [   196  14608  32987  28045  20032  14093   9851   6927  15808   3850]
 [   162  10802  25943  23064  16978  12061   8772   6198  14237   3615]
 [   367  30868  81809  78729  61557  45667  33945  25012  58947  16754]
 [   101  16367  54860  63042  55037  44428  34462  26588  71740  27032]]
2024-04-28 03:30:13,708 - INFO - Epoch: 5 | Train Loss: 7790.4410
2024-04-28 03:32:37,558 - INFO - Custom bins confusion matrix:
2024-04-28 03:32:37,558 - INFO - [[ 5744 72290 65367 33285 18619  8323  3527  1503   681     1]
 [ 2361 33176 40965 27534 17405  9354  4367  1657   821     0]
 [  924 16253 24063 19489 14670  9139  4289  1604   921     0]
 [  427  8703 15063 14085 11572  8665  4134  1458   880     0]
 [  227  4932 10256 10140  9126  7406  4030  1442   793     0]
 [  176  2879  6849  7647  7175  6092  3849  1352   770     0]
 [   94  1906  5019  5826  5636  5324  3339  1210   778     0]
 [   43  1334  3978  4377  4827  4632  3070  1203   694     0]
 [   83  3138 11084 14186 17574 18206 13475  5588  3296     0]
 [   82  1492  5503  7073 11329 17081 15166 10106  4209     0]]
2024-04-28 03:32:39,447 - INFO - Epoch: 5 | Validation Loss: 3401.8681
2024-04-28 03:44:10,410 - INFO - Custom bins confusion matrix:
2024-04-28 03:44:10,411 - INFO - [[ 17029 323927 320729 145336  68744  35890  19853  12082  21196   3772]
 [  7427 164326 201424 108490  58004  32758  19738  12555  22698   4163]
 [  3679  86598 123346  76442  44702  27250  17149  11138  21823   4286]
 [  2022  49889  81796  56746  35654  22475  14947   9972  20016   4266]
 [  1236  32658  57986  42826  28637  18902  12813   8790  18189   4164]
 [   763  22400  42942  34225  23408  16211  10935   7479  16400   3912]
 [   494  16022  33135  27889  19956  13760   9510   6795  15162   3674]
 [   332  11875  26047  22952  17144  11852   8502   5941  13728   3459]
 [   823  34225  82828  78504  61566  45061  32917  24088  57224  16419]
 [   356  18039  55781  63427  55244  43759  34031  26013  70169  26838]]
2024-04-28 03:44:15,207 - INFO - Epoch: 6 | Train Loss: 7730.6731
2024-04-28 03:46:40,145 - INFO - Custom bins confusion matrix:
2024-04-28 03:46:40,145 - INFO - [[   51 49418 92997 40382 15460  6354  2394  1105  1119    60]
 [   71 26072 57612 30194 13254  6012  2129  1050  1203    43]
 [   26 13482 34674 22166 11356  5488  1911  1028  1164    57]
 [   30  7335 22204 16625  9843  4802  1933  1014  1148    53]
 [    0  4094 15034 12693  8474  4168  1807   872  1165    45]
 [    0  2565 10426  9705  6775  3659  1727   849  1045    38]
 [   16  1721  7677  7417  5613  3352  1519   820   975    22]
 [   30  1333  5835  6052  4780  3071  1406   825   824     2]
 [   37  3325 18141 18284 19314 13403  6471  3495  4015   145]
 [    0  1773  8627 11025 17426 14046  8142  4625  6215   162]]
2024-04-28 03:46:42,150 - INFO - Epoch: 6 | Validation Loss: 3553.9025
2024-04-28 03:58:09,361 - INFO - Custom bins confusion matrix:
2024-04-28 03:58:09,362 - INFO - [[ 10967 278481 336652 164445  76588  39503  21925  12958  22798   4241]
 [  4596 144820 208400 116348  61225  34316  20361  12705  24079   4733]
 [  2274  76567 126880  80448  46604  27705  17414  11286  22427   4808]
 [  1370  44725  82913  58530  36691  23089  14948   9987  20847   4683]
 [   864  29154  58633  44082  29188  19006  12809   8852  18946   4667]
 [   524  20203  43122  34608  23722  16243  11089   7890  16938   4336]
 [   372  14451  33385  27966  20158  13727   9741   6904  15487   4206]
 [   277  10869  26262  22958  16979  12037   8573   5949  14111   3817]
 [   601  31264  81520  78589  61227  45258  33012  24614  59600  17970]
 [   206  16056  54628  62139  54793  43585  33925  26451  72320  29554]]
2024-04-28 03:58:14,020 - INFO - Epoch: 7 | Train Loss: 7642.4463
2024-04-28 04:00:36,867 - INFO - Custom bins confusion matrix:
2024-04-28 04:00:36,869 - INFO - [[    21  25233 104236  46309  18835   7488   3915   1942   1326     35]
 [    17  12698  59082  33985  16260   7971   3936   2167   1494     30]
 [     0   6214  33803  22575  13564   7483   4077   2024   1564     48]
 [     0   3225  20504  15896  11098   7027   3755   1950   1470     62]
 [     0   1734  13426  11553   8857   5942   3484   1800   1521     35]
 [     0   1109   8789   8846   6742   4923   3251   1593   1509     27]
 [     0    717   6312   6905   5322   4000   2972   1563   1317     24]
 [     0    565   4627   5640   4464   3540   2516   1641   1142     23]
 [     0   1425  12988  18151  15822  14838  10391   7484   5529      2]
 [     0    271   6841   8244  11849  14540  12443   8842   9011      0]]
2024-04-28 04:00:38,681 - INFO - Epoch: 7 | Validation Loss: 3397.8158
2024-04-28 04:12:11,293 - INFO - Custom bins confusion matrix:
2024-04-28 04:12:11,294 - INFO - [[ 21742 300256 337547 149789  68640  34656  19545  11779  20512   4092]
 [ 10388 158645 207369 109185  56962  31745  19062  11783  22004   4440]
 [  5339  84761 127619  76039  43329  26377  16521  10705  21145   4578]
 [  2964  50129  84057  56461  34741  21722  14300   9548  19288   4573]
 [  1851  33007  59285  43190  28044  18245  12162   8223  17865   4329]
 [  1222  23025  43424  34029  23034  15758  10609   7391  16171   4012]
 [   896  16870  33613  27493  19499  13441   9490   6578  14613   3904]
 [   662  12438  26852  22939  16401  11431   8092   5784  13505   3728]
 [  1560  36214  84352  78435  59448  43913  32030  23525  56490  17688]
 [   729  19223  57817  62807  53823  42331  32999  25180  69499  29249]]
2024-04-28 04:12:16,011 - INFO - Epoch: 8 | Train Loss: 7552.8144
2024-04-28 04:14:38,772 - INFO - Custom bins confusion matrix:
2024-04-28 04:14:38,773 - INFO - [[  720 89646 74838 24795 10563  4805  2140   962   763   108]
 [  465 47889 50079 21047  9904  4564  2008   861   723   100]
 [  265 24754 32754 17026  8674  4558  1678   827   679   137]
 [  166 14088 22357 12787  8164  4236  1672   652   709   156]
 [   56  8393 15803 10311  6841  3826  1532   737   728   125]
 [   44  5220 11285  8210  5705  3405  1480   719   643    78]
 [   43  3688  8285  6558  4852  3184  1278   627   564    53]
 [   53  2678  6420  5701  4175  2740  1290   594   439    68]
 [   75  7659 19793 18739 17317 11602  5805  2753  2554   333]
 [    0  3982 10274 11419 17323 14294  7478  3368  3663   240]]
2024-04-28 04:14:40,639 - INFO - Epoch: 8 | Validation Loss: 3720.1679
2024-04-28 04:26:13,930 - INFO - Custom bins confusion matrix:
2024-04-28 04:26:13,931 - INFO - [[ 25101 312071 317718 148108  69525  36624  20538  12584  21995   4294]
 [ 10926 159792 201452 110335  57690  32469  19656  12256  22575   4432]
 [  4971  83614 125500  77468  44754  26753  16775  10930  21236   4412]
 [  2799  48554  83106  57365  35463  22395  14570   9537  19679   4315]
 [  1690  31499  58349  44091  28860  18581  12445   8516  17764   4406]
 [  1011  21544  43423  34637  23564  15970  10810   7573  16154   3989]
 [   733  15595  33316  28293  19856  13815   9520   6624  14866   3779]
 [   522  11565  26517  23410  16977  11808   8305   5878  13268   3582]
 [  1229  31756  83071  79468  61525  45561  33228  23954  56991  16872]
 [   385  16127  55042  63199  54859  43610  34008  26282  71297  28848]]
2024-04-28 04:26:18,815 - INFO - Epoch: 9 | Train Loss: 7208.1788
2024-04-28 04:28:42,838 - INFO - Custom bins confusion matrix:
2024-04-28 04:28:42,839 - INFO - [[  989 70897 80551 31071 13430  6268  3091  1673  1225   145]
 [  558 33326 51080 25652 13024  7271  3286  1854  1483   106]
 [  329 16063 30888 18704 11179  7270  3322  1833  1608   156]
 [  180  8378 19706 13967  8958  6883  3357  1798  1620   140]
 [   16  4613 13835 10241  7196  5819  3258  1639  1617   118]
 [    2  2735  9274  7962  5946  4598  3056  1575  1515   126]
 [    9  2000  6635  6023  4794  3915  2724  1490  1464    78]
 [   24  1330  5097  4885  4122  3411  2482  1537  1199    71]
 [   16  3136 15215 15521 15101 13759 10747  7231  5614   290]
 [    0  1540  7110  7858 11736 13506 11704  8098 10410    79]]
2024-04-28 04:28:44,558 - INFO - Epoch: 9 | Validation Loss: 3562.3826
2024-04-28 04:40:17,898 - INFO - Custom bins confusion matrix:
2024-04-28 04:40:17,899 - INFO - [[ 34948 313338 317177 145923  67533  34198  19436  11808  20448   3749]
 [ 14973 158062 201457 109583  57411  32308  19153  12013  22259   4364]
 [  7011  82194 124644  77616  44486  26689  16893  10974  21381   4525]
 [  3921  47670  81863  57764  35534  22355  14708   9677  19873   4418]
 [  2347  30892  58112  43538  28666  18819  12684   8664  18192   4287]
 [  1472  21135  42614  34790  23596  15691  11118   7627  16542   4090]
 [  1039  15086  33139  27981  19915  13854   9599   6693  15243   3848]
 [   741  11178  26376  22799  16727  11946   8337   6001  13945   3782]
 [  1710  31278  81782  78495  61063  44872  33125  24735  58827  17768]
 [   612  16163  53403  61407  54124  43786  33853  26255  72783  31271]]
2024-04-28 04:40:22,779 - INFO - Epoch: 10 | Train Loss: 7021.6154
2024-04-28 04:42:47,161 - INFO - Custom bins confusion matrix:
2024-04-28 04:42:47,162 - INFO - [[  678 61526 86930 33281 14237  6661  3123  1622  1224    58]
 [  426 28344 53597 27142 13600  7569  3610  1728  1590    34]
 [  260 13392 31961 19505 11482  7389  3784  1681  1803    95]
 [  152  6941 20063 14240  9313  6941  3676  1859  1704    98]
 [   17  3681 14023 10451  7297  5815  3586  1712  1707    63]
 [    0  2201  9456  7963  5927  4630  3284  1650  1628    50]
 [    5  1578  6793  6088  4726  3951  2840  1533  1566    52]
 [   25  1139  5072  4913  4014  3513  2518  1524  1381    59]
 [   11  2614 15163 15128 15165 13464 11038  7305  6508   234]
 [    0  1303  7084  8138 11064 12826 11379  8076 12061   110]]
2024-04-28 04:42:48,926 - INFO - Epoch: 10 | Validation Loss: 3390.4604
2024-04-28 04:54:18,832 - INFO - Custom bins confusion matrix:
2024-04-28 04:54:18,833 - INFO - [[ 33277 297792 328402 150934  68930  34780  19237  11373  20154   3679]
 [ 13949 147863 205920 113376  59185  32842  19703  12110  22472   4163]
 [  6561  76486 125715  79931  45931  27465  17124  11058  21751   4391]
 [  3634  43698  82152  58845  36311  23054  15104  10088  20490   4407]
 [  2044  27891  57896  45174  29408  19255  12898   8699  18674   4262]
 [  1232  19043  42919  34980  24355  16264  11303   7710  16790   4079]
 [   945  13646  32707  28192  20503  14297  10032   6800  15406   3869]
 [   676   9847  25946  23330  16889  12368   8805   6081  14151   3739]
 [  1545  28041  80375  78924  61868  45824  34005  24913  60595  17565]
 [   562  14293  51831  61060  54629  43702  34678  26865  75362  30675]]
2024-04-28 04:54:23,574 - INFO - Epoch: 11 | Train Loss: 7138.4211
2024-04-28 04:56:47,508 - INFO - Custom bins confusion matrix:
2024-04-28 04:56:47,509 - INFO - [[  932 59939 86411 34901 14823  6777  2954  1480  1081    42]
 [  561 27774 52745 28285 14320  7500  3471  1599  1358    27]
 [  358 13133 30923 20486 11911  7739  3605  1549  1574    74]
 [  177  6711 19229 15099  9705  7215  3679  1616  1458    98]
 [   21  3593 13286 11157  7784  6027  3427  1549  1442    66]
 [    2  2118  8650  8827  6187  4888  3191  1528  1344    54]
 [   19  1422  6174  6812  4992  4137  2756  1474  1305    41]
 [   36  1084  4657  5310  4248  3606  2595  1491  1093    38]
 [   26  2237 13988 16806 16023 13512 11242  7229  5425   142]
 [    0  1038  6525  8726 11915 13247 11710  8046 10759    75]]
2024-04-28 04:56:49,615 - INFO - Epoch: 11 | Validation Loss: 3505.2637
2024-04-28 05:08:20,703 - INFO - Custom bins confusion matrix:
2024-04-28 05:08:20,703 - INFO - [[ 45659 295072 309745 152396  71932  36522  20585  12153  20708   3786]
 [ 19554 145485 196232 113972  61196  34299  20426  12806  23406   4207]
 [  8981  75017 119500  80141  47632  28279  18061  11871  22516   4415]
 [  4850  42812  78009  59178  37672  23829  15572  10335  21087   4439]
 [  2809  27525  54581  45005  29853  19852  13657   9071  19481   4367]
 [  1705  18493  40449  34996  24889  16665  11643   7981  17654   4200]
 [  1122  13232  30881  28631  20661  14527  10097   7225  16037   3984]
 [   773   9604  24444  23278  17642  12496   8884   6309  14647   3755]
 [  1906  26963  75112  78542  62775  46708  34838  25692  62760  18359]
 [   735  13463  47817  59591  53643  44516  35196  27256  78219  33221]]
2024-04-28 05:08:25,570 - INFO - Epoch: 12 | Train Loss: 6774.5470
2024-04-28 05:10:49,623 - INFO - Custom bins confusion matrix:
2024-04-28 05:10:49,625 - INFO - [[ 1721 57543 84269 37069 15371  7600  3180  1469  1106    12]
 [  917 26332 49904 29905 14968  8672  3838  1675  1429     0]
 [  485 12521 28558 21418 12242  8383  4309  1834  1572    30]
 [  211  6490 17538 15403  9904  7559  4377  1891  1590    24]
 [   44  3483 11895 11328  7902  6355  3765  1932  1642     6]
 [   44  1901  7810  8749  6296  5072  3507  1814  1596     0]
 [   34  1258  5545  6677  5128  4318  2786  1804  1582     0]
 [   45   962  4014  5445  4174  3716  2698  1676  1428     0]
 [   29  1973 11622 17052 15806 13446 11835  7615  7246     6]
 [    0   862  5430  8517 10825 12082 12745  8155 13400    25]]
2024-04-28 05:10:51,560 - INFO - Epoch: 12 | Validation Loss: 3297.8755
2024-04-28 05:22:28,304 - INFO - Custom bins confusion matrix:
2024-04-28 05:22:28,305 - INFO - [[ 58663 285263 310755 152882  70881  35684  19623  11594  19573   3640]
 [ 26039 142245 193912 115157  61210  33767  19927  12631  22544   4151]
 [ 12341  73712 117832  80802  47590  28210  17709  11528  22191   4498]
 [  6595  42546  77394  58966  37598  23611  15388  10058  20950   4677]
 [  3986  27056  54086  45029  30255  19793  13276   8919  19205   4596]
 [  2440  18297  39787  35259  24526  16722  11311   8065  17798   4470]
 [  1711  13110  30282  28444  20734  14493  10181   6987  16268   4187]
 [  1305   9400  23928  23179  17261  12566   8922   6366  14855   4050]
 [  2989  26551  73933  77613  62377  46891  35094  25866  63289  19052]
 [   915  12750  47078  59004  53013  43808  35043  27278  79078  35690]]
2024-04-28 05:22:33,156 - INFO - Epoch: 13 | Train Loss: 7144.9567
2024-04-28 09:26:43,344 - INFO - Custom bins confusion matrix:
2024-04-28 09:26:43,346 - INFO - [[ 26741 100733  44420  17984   8488   4739   2507   1588   2014    126]
 [ 12489  53422  32122  17016   9406   5611   2963   1916   2554    141]
 [  5664  28894  20971  13275   8635   5318   3246   2257   2866    226]
 [  2862  16829  14006  10172   7284   5050   3204   2498   2878    204]
 [  1367  10714  10140   7554   5966   4270   2857   2236   3073    175]
 [   811   6609   7349   5905   4887   3571   2541   2007   2935    174]
 [   619   4323   5628   4731   3940   2983   2318   1736   2742    112]
 [   532   3171   4305   3776   3348   2753   2103   1612   2467     91]
 [   892   8800  12727  13601  12097  10676   8494   7113  11738    492]
 [   441   3986   6944   7799   7995   9299   9005   7970  17654    948]]
2024-04-28 09:26:45,308 - INFO - Epoch: 13 | Validation Loss: 3416.9135
2024-04-28 09:54:28,156 - INFO - Custom bins confusion matrix:
2024-04-28 09:54:28,156 - INFO - [[ 57837 305305 306282 146420  66431  34236  18721  11005  18974   3347]
 [ 23874 147928 195848 113099  59868  33150  19666  12135  21918   4097]
 [ 11074  75128 120003  79840  47079  27991  17652  11171  22037   4438]
 [  5769  42421  78453  58770  37266  23903  15444  10167  21006   4584]
 [  3299  26724  54722  44864  30046  19808  13514   9130  19622   4472]
 [  1959  17946  40072  35264  24751  17035  11587   8210  17497   4354]
 [  1259  12819  30643  28412  20455  14612  10298   7197  16444   4258]
 [   894   9167  24229  23301  17456  12492   8999   6423  14818   4053]
 [  1995  24191  73125  78287  63266  47618  35430  26204  64067  19472]
 [   642  10804  44197  59099  54508  45226  35400  28206  79468  36107]]
2024-04-28 09:54:41,758 - INFO - Epoch: 14 | Train Loss: 6693.7022
2024-04-28 10:00:53,418 - INFO - Custom bins confusion matrix:
2024-04-28 10:00:53,422 - INFO - [[ 1927 57104 80565 39251 16335  7114  3753  1805  1394    92]
 [ 1035 25716 47077 31397 15811  8261  4395  1936  1944    68]
 [  527 12395 26373 22158 12826  8002  4654  2136  2112   169]
 [  242  6406 16012 15937  9988  7201  4593  2269  2193   146]
 [   48  3447 10824 11454  8094  5841  4036  2293  2196   119]
 [   57  1929  6915  8635  6530  4866  3500  2069  2179   109]
 [   61  1241  4906  6445  5513  3826  3053  1863  2151    73]
 [   51   993  3536  5102  4460  3472  2759  1728  1965    92]
 [   52  1794 10134 16576 15652 13000 11069  8253  9864   236]
 [    4   998  5018  8017  9650 10659 12180  9891 15334   290]]
2024-04-28 10:00:58,861 - INFO - Epoch: 14 | Validation Loss: 3514.4450
2024-04-28 10:00:59,009 - INFO - Experiment ended. Checkpoints stored =)
