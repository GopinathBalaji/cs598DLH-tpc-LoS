2024-04-12 14:15:19,493 - INFO - Config:
2024-04-12 14:15:19,509 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/MIMIC/LoS/Transformer",
    "batch_size": 32,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "d_model": 16,
    "dataset": "MIMIC",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "Transformer",
    "feedforward_size": 256,
    "intermediate_reporting": false,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00017,
    "loss": "msle",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "n_epochs": 15,
    "n_heads": 2,
    "n_layers": 6,
    "name": "Transformer",
    "no_diag": true,
    "no_exp": false,
    "no_labs": false,
    "no_mask": false,
    "percentage_data": 100.0,
    "positional_encoding": false,
    "save_results_csv": true,
    "seed": 4167684856,
    "shuffle_train": false,
    "sum_losses": true,
    "task": "LoS",
    "trans_dropout_rate": 0
}
2024-04-12 14:15:19,868 - INFO - Experiment set up.
2024-04-12 14:15:19,978 - INFO - Transformer(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (trans_dropout): Dropout(p=0, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (transformer): TransformerEncoder(
    (input_embedding): Conv1d(204, 16, kernel_size=(1,), stride=(1,))
    (pos_encoder): PositionalEncoding()
    (trans_encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
      )
      (linear1): Linear(in_features=16, out_features=256, bias=True)
      (dropout): Dropout(p=0, inplace=False)
      (linear2): Linear(in_features=256, out_features=16, bias=True)
      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
  )
  (diagnosis_encoder): Linear(in_features=1, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=49, out_features=17, bias=True)
  (point_mort): Linear(in_features=49, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
2024-04-12 14:28:12,730 - INFO - Custom bins confusion matrix:
2024-04-12 14:28:12,730 - INFO - [[289437 469639 105868  42528  21821  12659   7771   5177  10626   3032]
 [134209 286093  91337  44358  24929  15193  10054   6835  14424   4151]
 [ 67770 172015  69097  36777  22197  14187   9332   6429  14442   4167]
 [ 38684 113192  52167  30364  19047  12471   8305   6128  13549   3876]
 [ 25134  79724  41524  25238  16089  10801   7193   5110  11831   3557]
 [ 18122  59224  33369  20819  13566   9264   6374   4460  10339   3138]
 [ 13090  46005  27924  17941  11796   8108   5542   4168   9114   2709]
 [ 10031  36923  23589  15231  10208   6964   4848   3550   8138   2350]
 [ 31399 119032  83408  56979  39586  27516  19355  13729  32834   9817]
 [ 21186  89289  74476  55464  40080  28499  20858  15131  36919  11755]]
2024-04-12 14:28:17,324 - INFO - Epoch: 0 | Train Loss: 147.2827
2024-04-12 14:30:31,314 - INFO - Custom bins confusion matrix:
2024-04-12 14:30:31,314 - INFO - [[76757 79485 28103 12100  6572  3789  1721   600   213     0]
 [30432 50023 24601 13140  9092  6034  2898  1073   347     0]
 [13354 28341 18470 11057  8669  6349  3537  1183   392     0]
 [ 6679 17669 12930  8597  7693  6252  3636  1130   401     0]
 [ 3716 11707  9302  6939  6561  5770  3226   810   321     0]
 [ 2303  7678  6835  5497  5560  5123  2785   677   331     0]
 [ 1456  5548  5537  4537  4434  4219  2407   696   298     0]
 [  933  4341  4497  3660  3741  3938  2146   600   302     0]
 [ 2290 11845 14332 14007 15545 16248  8462  2735  1166     0]
 [ 1164  6428  8616 11284 14979 14791  8792  4115  1872     0]]
2024-04-12 14:30:32,978 - INFO - Epoch: 0 | Validation Loss: 113.5162
2024-04-12 14:43:17,081 - INFO - Custom bins confusion matrix:
2024-04-12 14:43:17,091 - INFO - [[350905 410641 105314  44338  22053  12170   7355   4688   9048   2046]
 [133668 262919 101546  51653  28624  17053  10868   7148  14660   3444]
 [ 56517 157678  77945  43577  26219  16418  11006   7329  15760   3964]
 [ 27797 101463  58813  35655  22768  14725  10099   6917  15397   4149]
 [ 16321  69608  45607  29181  18905  12995   8879   6185  14535   3985]
 [ 10214  50194  36230  24424  16418  11169   7916   5591  12870   3649]
 [  6908  37827  29888  20739  14260   9657   6892   5006  11875   3345]
 [  4867  29355  24907  17675  12252   8646   6078   4248  10624   3180]
 [ 14069  89451  84673  64658  47018  34100  24555  17998  43506  13627]
 [  7074  59897  70987  60087  46270  34900  25860  19523  51100  17959]]
2024-04-12 14:43:21,459 - INFO - Epoch: 1 | Train Loss: 127.3287
2024-04-12 14:45:35,621 - INFO - Custom bins confusion matrix:
2024-04-12 14:45:35,621 - INFO - [[82860 74220 28531 13363  5577  2702  1297   507   282     1]
 [31420 47405 26040 15159  8576  4611  2696  1198   535     0]
 [13148 27234 19047 12454  8082  5577  3348  1660   802     0]
 [ 6377 16715 13498  9842  7017  5211  3712  1674   941     0]
 [ 3412 11091  9662  7586  5975  4811  3464  1573   778     0]
 [ 2128  7202  6973  6090  4910  4322  2955  1434   775     0]
 [ 1346  5181  5576  4760  3871  3612  2654  1367   765     0]
 [  898  3933  4280  3950  3326  3306  2494  1236   735     0]
 [ 2043 10554 14608 14074 13277 13106  9703  5715  3550     0]
 [ 1192  5059  8731 10298 10731 12829 10764  6715  5722     0]]
2024-04-12 14:45:37,334 - INFO - Epoch: 1 | Validation Loss: 108.1698
2024-04-12 14:58:20,591 - INFO - Custom bins confusion matrix:
2024-04-12 14:58:20,591 - INFO - [[345287 423874 103059  43706  21249  11192   6581   4240   7617   1753]
 [124303 269254 105738  52855  28977  16698  10492   6740  13400   3126]
 [ 50850 158993  80148  45356  26594  16583  10852   7291  15671   4075]
 [ 23944 100165  60560  37041  23542  15637   9917   7102  15507   4368]
 [ 13719  67627  46518  30067  19765  13373   9233   6452  15086   4361]
 [  8328  48342  36660  25207  16754  11554   8168   5759  13664   4239]
 [  5554  36100  29687  20950  14405  10361   7456   5108  12717   4059]
 [  3917  27681  24787  17921  12595   8685   6471   4564  11433   3778]
 [ 11146  81610  82084  64887  48282  35263  25880  19203  48449  16851]
 [  5540  52604  66669  58321  46036  35515  26975  21125  57379  23493]]
2024-04-12 14:58:25,086 - INFO - Epoch: 2 | Train Loss: 121.8801
2024-04-12 15:00:39,171 - INFO - Custom bins confusion matrix:
2024-04-12 15:00:39,171 - INFO - [[85680 74232 27025 12813  5352  2454  1083   407   293     1]
 [31122 48611 25247 15537  8510  4420  2318  1144   731     0]
 [12636 27763 18501 12990  8155  5359  3117  1766  1065     0]
 [ 5951 17085 13066 10129  6981  4968  3478  1956  1373     0]
 [ 3206 11185  9345  7884  5690  4584  3235  2008  1215     0]
 [ 2015  7241  6685  6112  4880  3895  3028  1730  1203     0]
 [ 1285  5246  5279  4695  3891  3225  2601  1651  1259     0]
 [  844  3853  4114  3893  3246  3112  2375  1460  1261     0]
 [ 1913 10353 13625 14066 12841 12001  9385  6306  6136     4]
 [ 1219  4531  8431  9896  9873 10919 10122  7827  9214     9]]
2024-04-12 15:00:40,863 - INFO - Epoch: 2 | Validation Loss: 105.0870
2024-04-12 15:13:25,050 - INFO - Custom bins confusion matrix:
2024-04-12 15:13:25,050 - INFO - [[355700 417424 102070  42944  20573  10984   6272   3856   7205   1530]
 [124353 268183 107303  53823  28938  16718  10115   6312  12785   3053]
 [ 50085 155999  82309  46562  27216  16840  10831   7300  15377   3894]
 [ 23200  96815  61846  38500  24072  15451  10374   7168  15750   4607]
 [ 13144  64832  46971  31367  20150  13633   9510   6680  15271   4643]
 [  7848  45743  36962  25786  17328  11998   8422   5891  14356   4341]
 [  5155  34098  29707  21553  14948  10652   7393   5307  13189   4395]
 [  3546  25743  24785  18180  12878   9083   6609   4925  12090   3993]
 [ 10474  74485  80440  65412  48951  36657  26744  19913  51787  18792]
 [  5396  46006  64074  57813  45992  36680  27405  21912  61529  26850]]
2024-04-12 15:13:29,529 - INFO - Epoch: 3 | Train Loss: 118.3114
2024-04-12 15:15:43,795 - INFO - Custom bins confusion matrix:
2024-04-12 15:15:43,795 - INFO - [[88120 73767 25139 12485  5533  2447  1061   404   382     2]
 [31348 49255 23971 15521  8795  4370  2259  1232   889     0]
 [12599 27655 18080 12970  8267  5349  3182  1869  1381     0]
 [ 5845 17215 12346 10107  7075  4900  3605  2078  1816     0]
 [ 3150 11280  8681  7820  5717  4589  3203  2172  1740     0]
 [ 1918  7297  6243  5996  4832  3942  2918  1993  1650     0]
 [ 1258  5227  4999  4495  3730  3227  2642  1817  1737     0]
 [  841  3820  3850  3788  3154  2957  2430  1601  1712     5]
 [ 1901  9922 12504 13863 12319 11593  9361  6831  8270    66]
 [ 1203  4347  7588  9225  9308  9824  9916  8187 12407    36]]
2024-04-12 15:15:45,489 - INFO - Epoch: 3 | Validation Loss: 103.0274
2024-04-12 15:28:29,561 - INFO - Custom bins confusion matrix:
2024-04-12 15:28:29,561 - INFO - [[361367 415226 100721  42755  20103  10638   5892   3743   6714   1399]
 [122868 268552 108783  54409  28765  16667  10061   6400  12398   2680]
 [ 48586 155003  83598  47522  27826  16942  10974   7256  14927   3779]
 [ 22320  95241  62377  39266  24546  15664  10483   7251  16091   4544]
 [ 12596  62718  47470  32019  20916  14082   9586   6757  15417   4640]
 [  7337  43950  36804  26096  17910  12524   8461   6241  14708   4644]
 [  4834  32100  29736  21949  15324  10901   7737   5585  13717   4514]
 [  3352  24319  24449  18384  13094   9421   6776   5039  12672   4326]
 [  9993  68703  78955  65934  50054  37299  27672  20784  54035  20226]
 [  5128  41754  60972  56947  45781  37433  28389  22651  65272  29330]]
2024-04-12 15:28:34,151 - INFO - Epoch: 4 | Train Loss: 115.4975
2024-04-12 15:30:48,055 - INFO - Custom bins confusion matrix:
2024-04-12 15:30:48,065 - INFO - [[87075 74978 24966 12303  5696  2429   983   494   414     2]
 [30325 49860 24260 15601  8963  4125  2210  1295  1000     1]
 [11965 27657 18244 13146  8549  5138  3146  1860  1641     6]
 [ 5483 17200 12308 10278  7185  4760  3455  2151  2164     3]
 [ 2916 11212  8597  7708  5955  4474  3178  2124  2181     7]
 [ 1769  7175  6168  5909  4908  3885  2862  1983  2128     2]
 [ 1165  5049  5004  4349  3866  3132  2589  1701  2275     2]
 [  808  3652  3870  3718  3183  2891  2241  1643  2141    11]
 [ 1868  9439 12306 13300 12321 11421  9044  6762 10063   106]
 [ 1157  4277  7028  8789  8921  9347  9442  7952 14998   130]]
2024-04-12 15:30:49,751 - INFO - Epoch: 4 | Validation Loss: 101.5038
2024-04-12 15:43:33,727 - INFO - Custom bins confusion matrix:
2024-04-12 15:43:33,727 - INFO - [[367351 411086 100214  42271  19949  10479   5944   3530   6410   1324]
 [123744 267149 109180  54863  29033  16820  10005   6152  12072   2565]
 [ 48151 152929  83953  48390  28627  17342  10875   7346  15198   3602]
 [ 21856  92625  63102  40016  25282  16255  10839   7269  16225   4314]
 [ 12245  60581  47412  32537  21585  14469   9999   6848  16006   4519]
 [  7109  42118  36899  26856  18284  12497   8984   6292  15136   4500]
 [  4584  30313  29632  22312  15737  11280   7886   5969  14128   4556]
 [  3207  22908  24044  18986  13545   9636   6926   5230  12973   4377]
 [  9641  63941  76664  65647  51085  38744  28844  21849  56763  20477]
 [  5106  37352  57215  55971  46551  37567  30071  23638  69201  30985]]
2024-04-12 15:43:38,169 - INFO - Epoch: 5 | Train Loss: 113.0482
2024-04-12 15:45:51,901 - INFO - Custom bins confusion matrix:
2024-04-12 15:45:51,901 - INFO - [[88772 73826 24255 12098  5780  2527  1052   530   498     2]
 [30783 49349 23892 15450  8850  4411  2337  1372  1195     1]
 [12180 27417 17762 12907  8548  5328  3353  1997  1858     2]
 [ 5568 17087 11901  9947  7140  4861  3649  2377  2452     5]
 [ 2972 11107  8346  7390  5822  4573  3247  2305  2584     6]
 [ 1786  7065  5954  5677  4744  3889  2976  2090  2603     5]
 [ 1141  5006  4746  4283  3675  3124  2662  1871  2621     3]
 [  819  3568  3729  3514  3077  2844  2315  1786  2493    13]
 [ 1845  9205 11938 12192 11730 11249  9378  7196 11781   116]
 [ 1201  4154  6605  7986  8472  8580  8987  8449 17408   199]]
2024-04-12 15:45:53,562 - INFO - Epoch: 5 | Validation Loss: 100.3406
2024-04-12 15:58:37,792 - INFO - Custom bins confusion matrix:
2024-04-12 15:58:37,807 - INFO - [[372809 407468  99721  41768  19718  10447   5828   3488   6118   1193]
 [124655 264857 110126  55328  29488  16776   9962   6222  11803   2366]
 [ 48027 151060  84701  49266  28705  17929  11026   7414  14822   3463]
 [ 21734  91269  63379  40421  25779  16370  11060   7615  16023   4133]
 [ 11959  59335  47451  33159  21894  14864  10154   6906  16124   4355]
 [  7064  40632  36818  27252  18539  12882   9098   6477  15433   4480]
 [  4465  29373  29339  22670  16051  11552   8205   5919  14394   4429]
 [  3117  21933  23960  19024  13678   9902   7372   5183  13328   4335]
 [  9534  60588  75050  65982  51677  39487  30118  22241  58228  20750]
 [  4977  34453  55381  55552  46991  38333  30955  24460  70702  31853]]
2024-04-12 15:58:42,250 - INFO - Epoch: 6 | Train Loss: 111.3224
2024-04-12 16:00:56,350 - INFO - Custom bins confusion matrix:
2024-04-12 16:00:56,350 - INFO - [[88738 74208 24127 11967  5684  2505  1063   512   534     2]
 [30431 49629 24064 15305  8777  4494  2364  1297  1278     1]
 [11988 27396 17912 12840  8526  5445  3291  1995  1957     2]
 [ 5389 17109 11911  9911  7137  4955  3684  2323  2562     6]
 [ 2934 10992  8399  7295  5801  4630  3208  2345  2741     7]
 [ 1750  6994  6011  5558  4693  3877  3012  2152  2734     8]
 [ 1130  4913  4776  4224  3609  3203  2594  1919  2756     8]
 [  824  3494  3760  3445  3043  2856  2289  1777  2653    17]
 [ 1836  8986 11828 11927 11387 11438  9306  7265 12554   103]
 [ 1187  4027  6496  7769  8190  8411  8640  8555 18510   256]]
2024-04-12 16:00:58,092 - INFO - Epoch: 6 | Validation Loss: 99.3670
2024-04-12 16:13:42,108 - INFO - Custom bins confusion matrix:
2024-04-12 16:13:42,108 - INFO - [[374424 406961 100341  41253  19551  10016   5544   3406   5983   1079]
 [124378 264197 111562  55386  29436  16788  10113   6014  11418   2291]
 [ 47794 149554  85838  49624  29062  17762  11134   7327  15057   3261]
 [ 21438  89814  63941  41038  26167  16861  11039   7510  15987   3988]
 [ 11689  58175  47418  33540  22547  14944  10309   7226  15997   4356]
 [  6911  39721  36865  27128  18955  13098   9403   6519  15630   4445]
 [  4396  28277  29654  22652  16303  11763   8346   6087  14514   4405]
 [  2987  21456  23745  18942  13839  10238   7464   5437  13509   4215]
 [  9374  58097  74370  65690  52263  40107  30248  23187  59273  21046]
 [  4986  32189  53637  54382  47137  39553  31026  24682  73322  32743]]
2024-04-12 16:13:46,937 - INFO - Epoch: 7 | Train Loss: 109.8936
2024-04-12 16:16:00,745 - INFO - Custom bins confusion matrix:
2024-04-12 16:16:00,745 - INFO - [[88470 75243 23497 11749  5494  2594  1139   558   595     1]
 [29964 50292 23880 14945  8541  4710  2546  1323  1438     1]
 [11702 27704 17732 12501  8428  5556  3453  2047  2228     1]
 [ 5202 17222 11842  9524  7036  5109  3726  2478  2841     7]
 [ 2839 11005  8357  7033  5620  4690  3290  2476  3034     8]
 [ 1703  6994  5994  5258  4618  3862  3067  2256  3026    11]
 [ 1107  4882  4693  4090  3507  3239  2589  1974  3038    13]
 [  813  3469  3675  3365  2926  2807  2309  1879  2886    29]
 [ 1792  8969 11493 11388 10792 11419  9372  7427 13859   119]
 [ 1170  4058  6214  7475  7736  7930  8442  8525 20112   379]]
2024-04-12 16:16:02,455 - INFO - Epoch: 7 | Validation Loss: 98.5721
2024-04-12 16:28:46,034 - INFO - Custom bins confusion matrix:
2024-04-12 16:28:46,034 - INFO - [[375738 406227 101133  41159  18897   9839   5395   3393   5703   1074]
 [124371 263194 112696  55730  29525  16454   9996   6070  11368   2179]
 [ 47379 148187  86996  50112  29506  17861  11291   7352  14594   3135]
 [ 21353  88471  64654  41403  26144  16952  11148   7621  16123   3914]
 [ 11616  56940  48162  33607  22595  15123  10363   7171  16356   4268]
 [  6778  38567  37275  27448  19276  13224   9319   6696  15629   4463]
 [  4351  27728  29552  22668  16601  11740   8425   6079  14841   4412]
 [  3005  20690  23769  19008  14214  10166   7469   5563  13702   4246]
 [  9202  56140  73271  65642  52511  40606  30943  23352  60770  21218]
 [  4907  30161  52143  53820  47290  39332  31763  25415  74813  34013]]
2024-04-12 16:28:50,527 - INFO - Epoch: 8 | Train Loss: 108.6826
2024-04-12 16:31:04,372 - INFO - Custom bins confusion matrix:
2024-04-12 16:31:04,372 - INFO - [[89265 75035 23967 11141  5235  2425  1107   569   595     1]
 [30121 50153 24613 14667  8193  4675  2516  1276  1426     0]
 [11745 27616 18335 12218  8267  5527  3424  2004  2215     1]
 [ 5269 17115 12303  9259  6904  5171  3724  2396  2836    10]
 [ 2862 10954  8656  6861  5479  4755  3262  2470  3039    14]
 [ 1718  6955  6302  5030  4570  3908  3005  2269  3009    23]
 [ 1124  4886  4851  3967  3443  3250  2589  1963  3034    25]
 [  821  3467  3768  3332  2860  2774  2299  1950  2849    38]
 [ 1827  8934 11754 11155 10546 11356  9497  7259 14178   124]
 [ 1189  4045  6313  7492  7414  7900  8266  8380 20540   502]]
2024-04-12 16:31:06,068 - INFO - Epoch: 8 | Validation Loss: 97.8159
2024-04-12 16:43:50,408 - INFO - Custom bins confusion matrix:
2024-04-12 16:43:50,410 - INFO - [[375613 407435 101979  39911  18656   9643   5314   3144   5833   1030]
 [123779 262849 114731  55454  29113  16531   9736   6185  11039   2166]
 [ 47116 147427  87900  50401  29173  17787  11530   7590  14498   2991]
 [ 21120  87494  65192  41623  26649  17160  11103   7491  16218   3733]
 [ 11426  55893  48793  33500  22684  15376  10502   7349  16502   4176]
 [  6720  37786  37348  27185  19443  13442   9636   6921  15921   4273]
 [  4273  27045  29634  22833  16487  11888   8663   6229  14921   4424]
 [  2960  20140  23760  18771  14356  10358   7747   5747  13737   4256]
 [  9160  54024  72822  65549  52940  41258  31201  23815  61381  21505]
 [  4856  28870  51384  52881  47115  39556  31978  25983  76127  34907]]
2024-04-12 16:43:54,782 - INFO - Epoch: 9 | Train Loss: 107.6883
2024-04-12 16:46:08,670 - INFO - Custom bins confusion matrix:
2024-04-12 16:46:08,670 - INFO - [[88900 76176 23809 10721  4987  2426  1157   543   620     1]
 [29586 51123 24836 14169  7949  4643  2544  1292  1498     0]
 [11443 28082 18570 11917  8035  5482  3431  2056  2333     3]
 [ 5166 17253 12493  9089  6654  5154  3769  2370  3026    13]
 [ 2799 10994  8810  6684  5403  4626  3331  2497  3186    22]
 [ 1673  7017  6338  4928  4505  3798  3011  2300  3184    35]
 [ 1092  4898  4919  3844  3394  3209  2625  1914  3203    34]
 [  809  3483  3796  3251  2761  2775  2272  1976  2994    41]
 [ 1769  9025 11774 10755 10404 11028  9498  7485 14742   150]
 [ 1178  4126  6224  7321  7218  7646  8053  8112 21431   732]]
2024-04-12 16:46:10,359 - INFO - Epoch: 9 | Validation Loss: 97.0982
2024-04-12 16:58:53,988 - INFO - Custom bins confusion matrix:
2024-04-12 16:58:53,988 - INFO - [[376204 406616 103189  39667  18535   9406   5370   3161   5453    957]
 [123770 261125 116422  55745  29389  16357   9818   6073  10824   2060]
 [ 46882 145699  89207  50260  29977  17960  11537   7470  14487   2934]
 [ 20899  85830  66256  41780  26536  17252  11475   7853  16198   3704]
 [ 11270  54597  49003  34176  22864  15636  10526   7393  16632   4104]
 [  6724  36561  37827  27446  19507  13877   9758   6895  15806   4274]
 [  4255  26032  29955  22724  16787  12291   8857   6271  14929   4296]
 [  2974  19305  23841  19063  14284  10721   7678   5656  14039   4271]
 [  9021  51882  72456  65382  53901  40954  31970  24108  62154  21827]
 [  4875  27035  50018  52045  47394  40120  32498  26217  77675  35780]]
2024-04-12 16:58:58,385 - INFO - Epoch: 10 | Train Loss: 106.6893
2024-04-12 17:01:12,271 - INFO - Custom bins confusion matrix:
2024-04-12 17:01:12,286 - INFO - [[87573 77984 23878 10170  4931  2442  1157   558   647     0]
 [28791 51951 25247 13748  7850  4613  2575  1313  1551     1]
 [11023 28451 18787 11732  7944  5469  3414  2067  2451    14]
 [ 4991 17438 12584  8885  6668  5064  3783  2416  3142    16]
 [ 2700 11000  9014  6423  5410  4525  3428  2473  3333    46]
 [ 1601  7027  6430  4782  4452  3820  2962  2293  3364    58]
 [ 1055  4907  5024  3673  3308  3252  2617  1915  3330    51]
 [  775  3495  3870  3179  2657  2782  2228  1975  3143    54]
 [ 1698  9138 11702 10522 10247 10772  9474  7565 15303   209]
 [ 1155  4126  6205  7213  6936  7574  7878  7899 22060   995]]
2024-04-12 17:01:13,938 - INFO - Epoch: 10 | Validation Loss: 96.5437
2024-04-12 17:13:56,628 - INFO - Custom bins confusion matrix:
2024-04-12 17:13:56,628 - INFO - [[376388 406971 103427  39482  18113   9324   5228   3079   5559    987]
 [123506 260436 117218  56176  29382  16540   9697   5973  10745   1910]
 [ 46715 144107  90761  50777  29893  18072  11402   7397  14464   2825]
 [ 20760  84623  67393  41737  27020  17226  11533   7778  16096   3617]
 [ 11126  53735  49625  34301  22869  15947  10739   7480  16424   3955]
 [  6533  35834  37929  27668  19959  13831   9739   7005  15967   4210]
 [  4219  25385  29952  23021  16792  12226   8877   6359  15104   4462]
 [  2897  18950  23580  19104  14559  10669   7938   5755  14131   4249]
 [  8875  50443  71325  65600  53464  42154  32011  24280  63689  21814]
 [  4723  25925  48484  52369  47293  40023  32881  26694  78858  36407]]
2024-04-12 17:14:01,142 - INFO - Epoch: 11 | Train Loss: 105.8647
2024-04-12 17:16:18,046 - INFO - Custom bins confusion matrix:
2024-04-12 17:16:18,046 - INFO - [[89743 76573 23765  9842  4755  2359  1150   534   618     1]
 [29592 51404 25476 13539  7702  4621  2504  1307  1490     5]
 [11346 28292 18904 11706  7793  5458  3366  2033  2437    17]
 [ 5167 17331 12817  8847  6519  5046  3710  2425  3106    19]
 [ 2796 10888  9234  6346  5377  4526  3361  2460  3310    54]
 [ 1649  6968  6570  4739  4423  3819  2959  2224  3369    69]
 [ 1101  4888  5159  3610  3267  3227  2610  1874  3338    58]
 [  804  3533  3892  3173  2602  2801  2177  1948  3170    58]
 [ 1777  9186 11803 10485 10176 10643  9365  7619 15321   255]
 [ 1208  4062  6270  7235  6974  7415  7813  7795 22068  1201]]
2024-04-12 17:16:19,740 - INFO - Epoch: 11 | Validation Loss: 95.9344
2024-04-12 17:29:03,117 - INFO - Custom bins confusion matrix:
2024-04-12 17:29:03,117 - INFO - [[377102 405693 104534  39309  18050   9238   5232   3067   5354    979]
 [123407 258410 119530  56393  29356  16414   9652   6004  10530   1887]
 [ 46474 142391  91892  51548  30092  18268  11446   7365  14093   2844]
 [ 20627  83052  68269  42369  26996  17488  11701   7803  16011   3467]
 [ 11169  52590  50067  34293  23631  15740  10830   7579  16394   3908]
 [  6662  34795  38058  28268  19813  14178   9791   7007  15943   4160]
 [  4133  24655  30009  23187  17286  12333   8805   6426  15226   4337]
 [  2831  18265  23836  19251  14799  10621   7816   5844  14170   4399]
 [  8758  48521  70975  65795  54024  42548  32386  24776  63340  22532]
 [  4733  24625  47554  51579  47462  40468  33465  26723  79863  37185]]
2024-04-12 17:29:07,605 - INFO - Epoch: 12 | Train Loss: 105.1128
2024-04-12 17:31:21,775 - INFO - Custom bins confusion matrix:
2024-04-12 17:31:21,775 - INFO - [[88699 77665 24003  9639  4750  2213  1183   546   640     2]
 [28829 51986 25832 13600  7506  4577  2480  1265  1554    11]
 [11020 28270 19334 11732  7768  5363  3350  1963  2528    24]
 [ 5027 17162 13140  8864  6527  4958  3669  2420  3194    26]
 [ 2688 10778  9429  6376  5330  4490  3354  2383  3459    65]
 [ 1561  6889  6713  4846  4351  3753  2911  2195  3485    85]
 [ 1057  4817  5271  3612  3214  3255  2551  1864  3411    80]
 [  746  3498  3947  3170  2607  2728  2181  1918  3299    64]
 [ 1698  9040 11885 10424 10160 10475  9228  7619 15737   364]
 [ 1183  4024  6157  7328  6961  7278  7592  7555 22397  1566]]
2024-04-12 17:31:23,499 - INFO - Epoch: 12 | Validation Loss: 95.3960
2024-04-12 17:44:06,351 - INFO - Custom bins confusion matrix:
2024-04-12 17:44:06,351 - INFO - [[378926 403349 105758  39102  17878   9193   5144   3006   5292    910]
 [123379 255813 121960  56938  29645  16197   9458   5825  10578   1790]
 [ 46351 140427  93654  51827  30064  18307  11581   7651  13800   2751]
 [ 20484  81562  69184  42919  27102  17603  11858   7783  15775   3513]
 [ 11084  51242  50555  35149  23454  16092  10887   7610  16352   3776]
 [  6531  34026  38304  28576  20106  13973   9912   7183  15934   4130]
 [  4165  23793  30257  23106  17439  12526   9004   6463  15332   4312]
 [  2884  17518  23910  19355  14863  10755   8085   5923  14227   4312]
 [  8704  46818  70561  66152  54440  42423  33017  24637  64303  22600]
 [  4800  23288  46256  51249  47681  40810  33316  26885  81149  38223]]
2024-04-12 17:44:10,812 - INFO - Epoch: 13 | Train Loss: 104.3237
2024-04-12 17:46:24,667 - INFO - Custom bins confusion matrix:
2024-04-12 17:46:24,667 - INFO - [[90410 76497 23723  9602  4674  2128  1163   520   619     4]
 [29387 51712 25739 13821  7421  4458  2386  1204  1497    15]
 [11324 28053 19485 11829  7816  5272  3235  1883  2423    32]
 [ 5182 17030 13257  9023  6536  4925  3588  2312  3102    32]
 [ 2791 10669  9537  6522  5356  4497  3233  2285  3387    75]
 [ 1617  6802  6759  4960  4382  3732  2902  2099  3437    99]
 [ 1089  4748  5307  3647  3315  3276  2472  1820  3364    94]
 [  780  3461  3947  3234  2651  2728  2146  1872  3269    70]
 [ 1759  8966 11744 10623 10422 10461  9091  7466 15611   487]
 [ 1239  3953  6140  7369  7143  7381  7429  7335 22182  1870]]
2024-04-12 17:46:26,355 - INFO - Epoch: 13 | Validation Loss: 94.8749
2024-04-12 17:59:09,649 - INFO - Custom bins confusion matrix:
2024-04-12 17:59:09,649 - INFO - [[379074 402558 106869  39167  17683   9060   5066   2983   5224    874]
 [123219 254509 123669  57173  29387  16267   9502   5817  10267   1773]
 [ 46392 138529  95110  52673  30132  18450  11428   7381  13719   2599]
 [ 20404  80223  69830  43893  27306  17763  11458   7774  15759   3373]
 [ 10990  50415  50903  35331  23891  16039  10971   7644  16238   3779]
 [  6513  33392  38552  28553  20104  14486  10052   7190  15693   4140]
 [  4127  23396  30181  23492  17476  12722   9079   6421  15219   4284]
 [  2893  16957  24035  19623  14932  11071   8046   5794  14148   4333]
 [  8685  45291  69727  66188  54685  43176  33259  24940  64876  22828]
 [  4769  22464  45048  51002  47809  40783  33194  27401  81798  39389]]
2024-04-12 17:59:14,094 - INFO - Epoch: 14 | Train Loss: 103.6838
2024-04-12 18:01:43,890 - INFO - Custom bins confusion matrix:
2024-04-12 18:01:43,890 - INFO - [[91175 76513 23272  9321  4580  2099  1189   542   640     9]
 [29547 52015 25478 13672  7367  4413  2380  1227  1515    26]
 [11297 28389 19273 11625  7852  5263  3234  1889  2490    40]
 [ 5172 17210 13104  8895  6579  4909  3519  2333  3225    41]
 [ 2792 10751  9476  6399  5377  4490  3149  2315  3516    87]
 [ 1590  6843  6719  4930  4363  3637  2900  2170  3517   120]
 [ 1076  4783  5293  3617  3288  3240  2444  1806  3467   118]
 [  771  3505  3886  3212  2649  2657  2160  1813  3416    89]
 [ 1753  9079 11547 10417 10330 10439  9024  7412 15987   642]
 [ 1235  4009  6092  7030  7359  7357  7181  7171 22403  2204]]
2024-04-12 18:01:45,634 - INFO - Epoch: 14 | Validation Loss: 94.5010
2024-04-12 18:01:49,053 - INFO - Experiment ended. Checkpoints stored =)
